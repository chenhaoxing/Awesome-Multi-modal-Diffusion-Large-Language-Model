# Awesome-Multi-modal-Diffusion-Large-Language-Model
‚ú®‚ú®Latest Papers on Multimodal dLLM and Related Areas

---

## Contents
- [Survey](#survey)
- [Leaderboard](#leaderboard)
- [Vision-language model](#vision-language-model)
- [Unified Generation and Understanding](#unified-generation-and-understanding)

---

## Survey

---

## Leaderboard

### üèÜ Understanding
| Model | Params | MMMU-val| MMMU-Pro-vision | MMStar | POPE  | MME-P| MMBench | MathVista | MMVet | SEEDBench | 
| :----------- |:----------- |:----------- |:----------- |:----------- |:-----------|:----------- |:-----------|:----------- |:----------- |:----------- |
| LLaDA-V|8B | 48.6 | 18.6 | 60.1 |- |1507 |- |- |- |74.8 |
| LaViDa |8B|43.3 | - |- |- |1365.6 |70.5 |44.8 |- |- |

### üèÜ Generation

---

## Vision-language model
| Title                                                        | Venue         | Year | Comment                                                     | Code                                                         |
| :----------------------------------------------------------- | :-----------: | :--: | :----------------------------------------------------------- | :----------------------------------------------------------- |
| [LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning](https://arxiv.org/pdf/2505.16933)| arXiv   | 2025 | First dVLM; based on LLaDA-8B-Base| [Code](https://ml-gsai.github.io/LLaDA-V-demo/) |
| [LaViDa: A Large Diffusion Language Model for Multimodal Understanding](https://arxiv.org/pdf/2505.16839)| arXiv   | 2025 |several training tricks: complementary masking, Prefix-DLM, and timestep shifting; based on LLaDA-8B-Base| [Code](https://github.com/jacklishufan/LaViDa) |

---

## Unified Generation and Understanding
