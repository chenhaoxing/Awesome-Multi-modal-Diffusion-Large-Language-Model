# Awesome-Multi-modal-Diffusion-Large-Language-Model
✨✨Latest Papers on Multimodal dLLM and Related Areas

---

## Contents
- [Survey](#survey)
- [Leaderboard](#leaderboard)
- [Vision-language model](#vision-language-model)
- [Unified Generation and Understanding](#unified-generation-and-understanding)

---

## Survey

---

## Leaderboard

### Understanding
| Model | Params ｜ MMMU-val| MMMU-Pro-vision | MMStar | POPE  | MME-P| MMBench | MathVista | MMVet | SEEDBench | 
| :----------- |:-----------: |:-----------: |:-----------: |:-----------: |:-----------: |:-----------: |:-----------: |:-----------: |:-----------: |
| LLaDA-V|8B | 48.6 | 18.6 | 60.1 |- |1507 |- |- |- |74.8 |

### Generation

---

## Vision-language model
| Title                                                        | Venue         | Year | Comment                                                     | Code                                                         |
| :----------------------------------------------------------- | :-----------: | :--: | :----------------------------------------------------------- | :----------------------------------------------------------- |
| [LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning](https://arxiv.org/pdf/2505.16933)| arXiv   | 2025 | First dVLM; based on LLaDA-8B-Base| [Code](https://ml-gsai.github.io/LLaDA-V-demo/) |

---

## Unified Generation and Understanding
