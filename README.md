# Awesome-Multi-modal-Diffusion-Large-Language-Model
‚ú®‚ú®Latest Papers on Multimodal dLLM and Related Areas

---

## Contents
- [Survey](#survey)
- [Dataset](#dataset)
- [Competition](#competition)
- [Papers](#papers)

---

## üìú Survey

## üèÜ Leaderboard

### Understanding
| Model | MMMU-val| MMMU-Pro-vision | MMStar | POPE  | MME-P| MMBench | MathVista | MMVet | SEEDBench | 
| :----------- |:-----------: |:-----------: |:-----------: |:-----------: |:-----------: |:-----------: |:-----------: |:-----------: |:-----------: |
| LLaDA-V| 48.6 | 18.6 | 60.1 |- |1507 |- |- |- |74.8 |

### Generation


## üóÇÔ∏è Vision-language model
| Title                                                        | Venue         | Year | Comment                                                     | Code                                                         |
| :----------------------------------------------------------- | :-----------: | :--: | :----------------------------------------------------------- | :----------------------------------------------------------- |
| [LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning](https://arxiv.org/pdf/2505.16933)| arXiv   | 2025 | First dVLM; based on LLaDA-8B-Base| [Code](https://ml-gsai.github.io/LLaDA-V-demo/) |



## üìÑ Unified Generation and Understanding
